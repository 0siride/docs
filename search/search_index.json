{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hello, dear","text":""},{"location":"#hello-dear","title":"Hello, dear!","text":""},{"location":"#what-is-this","title":"What is this?","text":"<p> The Cheshire Cat is an open-source framework that allows you to develop intelligent agents on top of many Large Language Models (LLM). You can develop your custom AI architecture to assist you in a wide range of tasks.</p> <p>The Cheshire Cat embeds a long-term memory system to save the user's input locally and it answers knowing the context of previous conversations. You can also feed text documents in the Cat's memory system to enrich the agent's contextual information and ask it to retrieve them further in the conversation. The Cat currently supports <code>.txt</code>, <code>.pdf</code> and <code>.md</code> files.</p> <p>If you want the Cat to solve tailored tasks you can extend its capabilities writing Python plugins to execute custom functions or call external services (e.g. APIs and other models).</p> <p>If you want to build your custom AI architecture, the Cat can help you!</p> Cheshire Cat Features  Can use external tools  Can ingest documents (.txt, .pdf, .md)  Language model agnostic  Long term memory  Extendible via plugins in Python  100% dockerized"},{"location":"#currently-supported-llms","title":"Currently Supported LLMs","text":"<p>GPT3, ChatGPT, Cohere, HuggingFace Hub, HuggingFace, Azure OpenAI endpoints</p> Get in touch with us!  Discord Join our Discord server and don't forget to give the project a star! \u2b50 Thanks again!\ud83d\ude4f <p></p> <pre><code>\"Would you tell me, please, which way I ought to go from here?\"\n\"That depends a good deal on where you want to get to,\" said the Cat.\n\"I don't much care where--\" said Alice.\n\"Then it doesn't matter which way you go,\" said the Cat.\n\n(Alice's Adventures in Wonderland - Lewis Carroll)\n</code></pre> <p>Credits</p> <p>Logo image generated with MidJourney, prompted by Edgars Romanovskis</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#faq","title":"FAQ","text":""},{"location":"faq/#general","title":"General","text":""},{"location":"faq/#ive-found-the-cat-and-i-like-it-very-much-but-im-not-able-to-follow-your-instructions-to-install-it-on-my-machine-can-you-help","title":"I've found the Cat and I like it very much, but I'm not able to follow your instructions to install it on my machine. Can you help?","text":"<p>The Cheshire Cat is a framework to help developers to build vertical AIs: you will need some basic technical skills to follow our instructions. Please try to ask in the support channel in our discord server, and remember this is all volunteers effort: be kind! :)</p>"},{"location":"faq/#why-the-cat-does-not-default-to-some-open-llm-instead-of-chatgpt-or-gpt-3","title":"Why the Cat does not default to some open LLM instead of ChatGPT or GPT-3?","text":"<p>Our intention is not to depend on any specific LLM: the Cat does not have a preference about which LLM to use. Nonetheless, at the moment, OpenAI tools still provide the best results for your bucks. Decision is up to you.</p>"},{"location":"faq/#are-text-and-documents-sent-to-the-cat-safe-and-not-shared-with-anybody","title":"Are text and documents sent to the Cat safe and not shared with anybody?","text":"<p>Well, the local memory is safe and under your control, although embeddings and prompts are shared with your configured LLM, meaning you need to check how safe is the LLM. We plan to adopt local LLMs, at which point all your data will be under your control.</p>"},{"location":"faq/#basic-info","title":"Basic Info","text":""},{"location":"faq/#can-i-insert-a-long-article-into-the-chat","title":"Can I insert a long article into the chat?","text":"<p>Please avoid pasting long articles into the chat. Use Rabbit Hole to upload long texts instead: just click on the attachment icon in the chat input widget and upload your file.</p>"},{"location":"faq/#are-the-configured-llm-apis-used-to-instruct-the-cat-with-the-documents-im-going-to-upload","title":"Are the configured LLM APIs used to \"instruct\" the Cat with the documents I'm going to upload?","text":"<p>That's not exactly how it works: basically when you ask something to the Cat, we pass to the configured LLM a prompt with your actual question + data that can be useful to answer that question. Data can be parts of your documents or chat history. Please check our documentation for more details about how the Cat works for you.</p>"},{"location":"faq/#can-i-talk-to-the-cat-in-a-language-different-from-english","title":"Can I talk to the Cat in a language different from English?","text":"<p>Of course you can: just change the prompts in the Plugin folder accordingly, and take care not to mix languages to get best results.</p>"},{"location":"faq/#how-can-i-know-where-the-cat-gets-the-answers-id-like-to-know-if-its-using-the-files-i-uploaded-or-if-its-querying-the-configured-llm","title":"How can I know where the Cat gets the answers? I'd like to know if it's using the files I uploaded or if it's querying the configured LLM.","text":"<p>Just open the console in your browser to check the logs there. At some point soon, this information will end up in the user interface, but at the moment is behind the scenes.</p>"},{"location":"faq/#i-sent-to-the-cat-some-text-and-documents-i-wont-to-get-rid-of-how-can-i-do","title":"I sent to the Cat some text and documents I won't to get rid of, How can I do?","text":"<p>You can delete the <code>long_term_memory</code> folder and restart the Cat!</p>"},{"location":"faq/#errors","title":"Errors","text":""},{"location":"faq/#why-am-i-getting-the-error-ratelimiterror-in-my-browser-console","title":"Why am I getting the error <code>RateLimitError</code> in my browser console?","text":"<p>Please check if you have a valid credit card connected or if you have used up all the credits of your OpenAI trial period.</p>"},{"location":"faq/#everything-works-in-localhost-but-not-on-another-server","title":"Everything works in localhost but not on another server","text":"<p>You should configure ports in the <code>.env</code> file. Change according to your preferred host and ports:</p> <pre><code># Decide host and port for your Cat. Default will be localhost:1865\nCORE_HOST=anotherhost.com\nCORE_PORT=9000\n</code></pre>"},{"location":"faq/#docker-has-no-permissions-to-write","title":"Docker has no permissions to write","text":"<p>This is a matter with your docker installation or the user you run docker from.</p>"},{"location":"faq/#the-cat-seems-not-to-be-working-from-inside-a-virtual-machine","title":"The Cat seems not to be working from inside a Virtual Machine","text":"<p>In VirtualBox you can select Settings-&gt;Network, then choose NAT in the \"Attached to\" drop down menu. Select \"Advanced\" to configure the port forwarding rules. Assuming the guest IP of your VM is 10.0.2.15 (the default) and the ports configred in the .env files are the defaults, you have to set at least the following rule:</p> Rule name Protocol Host IP Host Port Guest IP Guest Port Rule 1 TCP 127.0.0.1 1865 10.0.2.15 1865 <p>If you want to work on the documentation of the Cat, you also have to add one rule for port 8000 which is used by <code>mkdocs</code>, and to configure <code>mkdocs</code> itself to respond to all requests (not only localhost as per the default).  </p>"},{"location":"faq/#customization","title":"Customization","text":""},{"location":"faq/#i-want-to-build-my-own-plugin-for-the-cat-what-should-i-know-about-licensing","title":"I want to build my own plugin for the Cat: what should I know about licensing?","text":"<p>Plugins are any license you wish, you can also sell them. The Cat core is GPL3, meaning you are free to fork and go on your own, but you are forced to open source changes to the core.</p>"},{"location":"faq/#port-1865-is-allowed-by-my-operating-system-andor-firewall","title":"Port 1865 is allowed by my operating system and/or firewall","text":"<p>Change the port as you wish in the <code>.env</code> file.</p> <pre><code># Decide host and port for your Cat. Default will be localhost:1865\nCORE_HOST=localhost\nCORE_PORT=9000\n</code></pre>"},{"location":"features/","title":"Features","text":"<p>TODO</p>"},{"location":"conceptual/llm/","title":"Language Models","text":""},{"location":"conceptual/llm/#language-models","title":"Language Models","text":"<p>A language model is a Deep Learning Neural Network trained on a huge amount of text data to perform different types of language tasks. Commonly, they are also referred to as Large Language Models (LLM). Language models comes in many architectures, size and specializations. The peculiarity of the Cheshire Cat is to be model-agnostic. This means it supports many different language models.</p> <p>By default, there are two classes of language models that tackles two different tasks.</p>"},{"location":"conceptual/llm/#completion-model","title":"Completion Model","text":"<p>This is the most known type of language models (see for examples ChatGPT, Cohere and many others). A completion model takes a string as input and generates a plausible answer by completion.</p> <p>Warning</p> <p>A LLM answer should not be accepted as-is, since LLM are subjected to hallucinations. Namely, their main goal is to generate plausible answers from the syntactical point of view. Thus, the provided answer could come from completely invented information.</p>"},{"location":"conceptual/llm/#embedding-model","title":"Embedding Model","text":"<p>This type of model takes a string as input and returns a vector as output. This is known as an embedding. Namely, this is a condensed representation of the input content. The output vector, indeed, embeds the semantic information of the input text.</p> <p>Despite being non-human readable, the embedding comes with the advantage of living in a Euclidean geometrical space. Hence, the embedding can be seen as a point in a multidimensional space, thus, geometrical operations can be applied to it. For instance, measuring the distance between two points can inform us about the similarity between two sentences.</p>"},{"location":"conceptual/llm/#language-models-flow","title":"Language Models flow","text":"<p>Developer documentation</p> <p>Language Models hooks</p> <p>Nodes with the  point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"conceptual/plugins/","title":"Plugins","text":""},{"location":"conceptual/plugins/#plugins","title":"Plugins","text":"<p>Plugins are add-ons that can be installed to extend and customize the Cheshire Cat. A plugin is nothing but a collection of hook and tools.</p>"},{"location":"conceptual/plugins/#hooks","title":"Hooks","text":"<p>The Cat uses functions knows as Hooks, which can be overridden, to customize the framework behavior in specific execution places. Hooks come with a priority property. The plugins manager takes care of collecting all the hooks, sorting and executing them in descending order of priority.</p>"},{"location":"conceptual/plugins/#tools","title":"Tools","text":"<p>Tools are custom Python functions that are called by the Agent. They come with a rich docstring upon with the Agent choose whether and which tool is the most suitable to fulfill the user's request. The list of available tools ends up in the Main Prompt, where the Agent receives instructions on how to structure its reasoning.</p> <p>Developer documentation</p> <ul> <li>How to write a plugin</li> <li>Hooks</li> <li>Tools</li> </ul>"},{"location":"conceptual/cheshire_cat/agent/","title":"The Agent","text":""},{"location":"conceptual/cheshire_cat/agent/#agent","title":"Agent","text":"<p>The Agent is the Cat's component that handles the tools execution. Sometimes a simple answer from the language model is not enough. For this reason, the Cat can exploit a set of custom tools coming from the plugins. The decision on whether and which action should be taken to fulfill the user's request is delegated to the Agent component.</p> <p>The Agent outlines a reasoning to take the aforementioned action. By default, the structure of the Agent's reasoning is defined in the instruction component of the Main Prompt.</p> <p>The list of available tools can be manually filtered with hooks to condition the Agent's decision.</p>"},{"location":"conceptual/cheshire_cat/agent/#agent-flow","title":"Agent flow","text":"<p>Developer documentation</p> <p>Agent hooks</p> <p>Nodes with the  point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"conceptual/cheshire_cat/mad_hatter/","title":"The Mad Hatter","text":""},{"location":"conceptual/cheshire_cat/mad_hatter/#mad-hatter","title":"Mad Hatter","text":"<p>The Mad Hatter is the Cat's plugins manager. It takes care of loading, prioritizing and executing plugins.</p> <p>Specifically, the Mad Hatter lists all available plugins in proper folder and sort their hooks in descending order of priority. When the Cat invokes them, it executes them following that order.</p> <p>Developer documentation</p> <ul> <li>How to write a plugin</li> <li>Hooks</li> <li>Tools</li> </ul>"},{"location":"conceptual/cheshire_cat/rabbit_hole/","title":"The Rabbit Hole","text":""},{"location":"conceptual/cheshire_cat/rabbit_hole/#rabbit-hole","title":"Rabbit Hole","text":"<p>The Rabbit Hole is the Cat's component that takes care of ingesting documents and storing them in the episodic memory. You can interact with it either through its endpoint, the GUI or a Python script.</p> <p>Currently supported file formats are: <code>.txt</code>, <code>.md</code>, <code>.pdf</code> or <code>.html</code> via web URL.</p>"},{"location":"conceptual/cheshire_cat/rabbit_hole/#rabbit-hole-flow","title":"Rabbit Hole flow","text":"<p>Developer documentation</p> <p>Rabbit Hole hooks</p> <pre><code>\nflowchart LR\nA[\"\ud83d\udcc4Document\"] --&gt; B[read];\nsubgraph rb [\"\ud83d\udc30RabbitHole\"]\nB[read] --&gt; C[\"\ud83e\ude9d\"];\nC[\"\ud83e\ude9d\"] --&gt; D[recursive split];\nD[\"\ud83e\ude9drecursive split\"] --&gt; E[\"\ud83e\ude9d\"];\nE[\"\ud83e\ude9d\"] --&gt; F[\"\ud83e\ude9dsummarization\"];\nF[\"\ud83e\ude9dsummarization\"] --&gt; G[\"\ud83e\ude9d\"];\nend\nG[\"\ud83e\ude9d\"] --&gt; H[\"\ud83d\udc18Episodic Memory\"] </code></pre> <p>Nodes with the  point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"conceptual/memory/long_term_memory/","title":"Long Term Memory","text":""},{"location":"conceptual/memory/long_term_memory/#long-term-memory","title":"Long Term Memory","text":"<p>The Cat's Long Term Memory (LTM) is made of two components:</p> <ul> <li>episodic memory, i.e. the context of documents uploaded to the Cat;</li> <li>declarative memory, i.e. the context of things the user said in the past.</li> </ul> <p>These are nothing but two vector databases where memories are stored in the form of vectors.</p> <p>You can interact with the LTM using its endpoint.</p> <p>By default, the Cat queries the LTM to retrieve the relevant context that is used to make up the Main Prompt.</p>"},{"location":"conceptual/memory/long_term_memory/#long-term-memory-flow","title":"Long Term Memory flow","text":"<p>Developer documentation</p> <p>Long Term Memory hooks</p> <pre><code>flowchart LR\n    subgraph LTM [\"\ud83d\udc18Long Term Memory\"]\n            direction TB\n            C[(Episodic)];\n            D[(Declarative)];\n    end\n    A[Query] --&gt; LTM; \n    LTM --&gt; E[\"\ud83e\ude9d\"];\n    E[\"\ud83e\ude9d\"] --&gt; wm[\"\u2699\ufe0f\ud83d\udc18Working Memory\"];</code></pre> <p>Nodes with the  point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"conceptual/memory/vector_memory/","title":"Vector Memory","text":""},{"location":"conceptual/memory/vector_memory/#vector-memory-collections","title":"Vector Memory Collections","text":"<p>The Vector Memory Collections are the lowest-level components of the Long Term Memory. These are particular databases that store the content in the form of geometrical vectors.</p> <p>A vector memory comes in the guise of a named collection of vectors and additional, optional metadata. The latter can be used to filter the search in the database. Each vector represents a memory. They are also called embeddings as they are the results of the text-to-vector conversion yielded by the embedder.</p> <p>Such databases are particularly useful because they allow to fetch relevant documents based on the vector similarity between a query and the stored embeddings.</p> <p>By default, Vector Memory Collections are created when the Cat is installed or after a complete memory swap.</p>"},{"location":"conceptual/memory/vector_memory/#vector-memory-collections-flow","title":"Vector Memory Collections flow","text":"<p>Developer documentation</p> <p>Vector Memory Collections hooks</p> <pre><code>flowchart LR\n    subgraph CAT [\"\ud83d\udc31Cheshire Cat\"]\n        direction LR\n        subgraph LTM [\"\ud83d\udc18Long Term Memory\"]\n            direction TB\n            C[(Episodic Memory)];\n            D[(Declarative Memory)];\n        end\n        A[\"First Memory\"] --&gt; H[\"\ud83e\ude9d\"];\n        H[\"\ud83e\ude9d\"] --&gt; C[(Episodic)];\n        H[\"\ud83e\ude9d\"] --&gt; D[(Declarative )];\n    end\n    E[First Installation] ----&gt; CAT;\n    F[Memory Swap] ----&gt; LTM;</code></pre> <p>Nodes with the  point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"conceptual/memory/working_memory/","title":"Working Memory","text":""},{"location":"conceptual/memory/working_memory/#working-memory","title":"Working Memory","text":"<p>The Working Memory is a handful component to store temporary data. For instance, it can be used to share data across plugins or, in general, across any function that get an instance of the Cat as an argument.</p> <p>By default, the Working Memory stores the chat history that ends up in the Main Prompt. Moreover, the Working Memory collects the relevant context that is fetched from the episodic and declarative memories in the Long Term Memory.</p>"},{"location":"conceptual/memory/working_memory/#working-memory-flow","title":"Working Memory flow","text":"<p>Developer documentation</p> <p>Long Term Memory hooks</p> <pre><code>flowchart LR\n    subgraph WM [\"\u2699\ufe0f\ud83d\udc18\ufe0fWorking Memory\"]\n            direction TB\n            CH[Chat History]\n            C[Episodic Memory];\n            D[Declarative Memory];\n    end\n    A[\"\ud83d\udc18Long Term Memory\"] --&gt; E[\"\ud83e\ude9d\"]; \n    E[\"\ud83e\ude9d\"] --&gt; WM;\n    CH --&gt; main_prompt[Main Prompt];\n    C --&gt; main_prompt[Main Prompt];\n    D --&gt; main_prompt[Main Prompt];</code></pre> <p>Nodes with the  point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"conceptual/prompts/hyde/","title":"HyDE prompt","text":""},{"location":"conceptual/prompts/hyde/#hypothetical-document-embedding-prompt","title":"Hypothetical Document Embedding Prompt","text":"<p>Hypothetical Document Embedding (HyDE)1 is a technique used to improve accuracy during similarity search among documents.</p> <p>Similarity search is the task of comparing text embeddings, i.e. the vector representation of documents, to find similarities between them, e.g. during document question-answering.</p> <p>As such, HyDE technique consists in asking the language model to generate a hypothetical answer that is used as query for the similarity search task. The idea behind this approach is that, during a question-answering task, using the hypothetical answer to search for similar documents would lead to better results than using the question itself as a search query.</p> <p>Specifically, the Cat uses this method to retrieve the relevant memories that are provided as context and end up in the Main Prompt. Moreover, it exploits a technique named few shots learning2. Namely, a method that consists in providing the language model a few examples in the prompt to get more accurate answers.</p> <p>By default, the HyDE prompt is the following:</p> <pre><code>hyde_prompt = \"\"\"You will be given a sentence.\nIf the sentence is a question, convert it to a plausible answer. \nIf the sentence does not contain a question, \njust repeat the sentence as is without adding anything to it.\nExamples:\n- what furniture there is in my room? --&gt; In my room there is a bed, \na wardrobe and a desk with my computer\n- where did you go today --&gt; today I was at school\n- I like ice cream --&gt; I like ice cream\n- how old is Jack --&gt; Jack is 20 years old\nSentence:\n- {input} --&gt; \"\"\"\n</code></pre>"},{"location":"conceptual/prompts/hyde/#hyde-flow","title":"HyDE flow","text":"<p>Developer documentation</p> <ul> <li>HyDE hooks</li> <li>Prompts hooks</li> </ul> <pre><code>flowchart LR\n    subgraph cat [\"\ud83d\udc31Cheshire Cat\"]\n    direction LR\n    hyde[\"\ud83e\ude9dHyDE prompt\"] --&gt; llm[Language Model];\n    llm --&gt;|generates|answer[Hypothetical Answer];\n    answer --&gt;|similarity search|ltm[\"\ud83d\udc18Long Term Memory\"];\n    ltm --&gt; context[Relevant Context];\n    context ---&gt;|inserted in|prompt[Main Prompt];\n    end\n    A[\"\ud83d\udc64User\"] ----&gt;|sends message|hyde;\n    A --&gt; prompt</code></pre> <p>Nodes with the  point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"conceptual/prompts/hyde/#references","title":"References","text":"<ol> <li> <p>Gao, L., Ma, X., Lin, J., &amp; Callan, J. (2022). Precise Zero-Shot Dense Retrieval without Relevance Labels. arXiv preprint arXiv:2212.10496.\u00a0\u21a9</p> </li> <li> <p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... &amp; Amodei, D. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901.\u00a0\u21a9</p> </li> </ol>"},{"location":"conceptual/prompts/main_prompt/","title":"Main Prompt","text":""},{"location":"conceptual/prompts/main_prompt/#main-prompt","title":"Main Prompt","text":"<p>The Main Prompt is the full set of instructions that is fed to the Agent. For instance, the prompt can be engineered to instruct the Cat to behave in a specific manner or to use the memory and the tools.</p> <p>This prompt is split in three parts:</p> <ul> <li>a prefix;</li> <li>the instructions;</li> <li>a suffix.</li> </ul> <p>Using such a complex prompt is an approach known as Retrieval Augmented Generation1. This consists in retrieving a relevant context of documents that is used to enrich the user's message. Specifically, the Cat exploits the Hypothetical Document Embedding2 (HyDE) technique to recall the relevant context from the Long Term Memory and, indeed, augment the Main Prompt. This is also augmented with the history of the recent conversation, a set of tools and the history the Agent's reasoning.</p> <p>In the following sections, we explain every prompt component.</p>"},{"location":"conceptual/prompts/main_prompt/#prefix","title":"Prefix","text":"<p>This is the first component. By default, it is:</p> <pre><code>prefix = \"\"\"You are the Cheshire Cat AI, an intelligent AI that passes the Turing test.\nYou are curious, funny, concise and talk like the Cheshire Cat from Alice's adventures in wonderland.\nYou answer Human using tools and context.\n# Tools\"\"\"\n</code></pre> <p>The Prefix describes who the AI is and how it is expected to answer the Human. This component ends with \"# Tools\" because the next part of the prompt (generated form the Agent) contains the list of Tools.</p>"},{"location":"conceptual/prompts/main_prompt/#instructions","title":"Instructions","text":"<p>This is the set of instructions that explain the Agent how to format its reasoning. The Agent uses such chain of thoughts to decide when and which tool is the most appropriate to fulfill the user's needs.</p> <p>By default, it is:</p> <pre><code>instructions = \"\"\"To use a tool, use the following format:\n\\```\nThought: Do I need to use a tool? Yes\nAction: the action to take /* should be one of [{tool_names}] */\nAction Input: the input to the action\nObservation: the result of the action\n\\```\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n\\```\nThought: Do I need to use a tool? No\n{ai_prefix}: [your response here]\n\\```\"\"\"\n</code></pre> <p>where the placeholder <code>{tool_names}</code> is replaced with the list of the available Python tools.</p>"},{"location":"conceptual/prompts/main_prompt/#suffix","title":"Suffix","text":"<p>This is the last component of the Main Prompt and, by default, is set as follows:</p> <pre><code>suffix = \"\"\"# Context\n## Context of things the Human said in the past:{episodic_memory}\n## Context of documents containing relevant information:{declarative_memory}\n## Conversation until now:{chat_history}\n - Human: {input}\n# What would the AI reply?\n{agent_scratchpad}\"\"\"\n</code></pre> <p>The purpose of this component is to provide the Agent with the context documents retrieved from the episodic and declarative memories, the recent conversation and the agent scratchpad, i.e. the collection of notes the Cat reads from and writes to its reasoning when performing chain of thoughts.</p>"},{"location":"conceptual/prompts/main_prompt/#main-prompt-flow","title":"Main Prompt flow","text":"<p>Developer documentation</p> <p>Main Prompt hooks</p> <pre><code>flowchart LR\n    subgraph MP [\"Main Prompt\"]\n%%        direction LR\n        Prefix[\"\ud83e\ude9dPrefix\"];\n        Instructions[\"\ud83e\ude9dInstructions\"];\n        Suffix[\"\ud83e\ude9dSuffix\"];    \n    end\n    subgraph CAT [\"\ud83d\udc31Cheshire Cat\"]\n        HyDE\n        subgraph LTM [\"\ud83d\udc18Long Term Memory\"]\n%%        direction\n        C[(Episodic)];\n        D[(Declarative)];\n    end\n    subgraph Agent [\"\ud83e\udd16Agent\"]\n        A[Agent Scratchpad];\n    end\n    end\n\n    U[\"\ud83d\udc64User\"] --&gt;|sends message|HyDE ---&gt; LTM[\"\ud83d\udc18Long Term Memory\"];\n    C --&gt; E[\"\ud83e\ude9d\"] ----&gt; Prefix;\n    D --&gt; E[\"\ud83e\ude9d\"] --&gt; Prefix;\n    A --&gt; Suffix;\n    MP -..-&gt;|fed back to|CAT -...-&gt; Answer</code></pre> <p>Nodes with the  point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"conceptual/prompts/main_prompt/#references","title":"References","text":"<ol> <li> <p>Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... &amp; Kiela, D. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33, 9459-9474.\u00a0\u21a9</p> </li> <li> <p>Gao, L., Ma, X., Lin, J., &amp; Callan, J. (2022). Precise Zero-Shot Dense Retrieval without Relevance Labels. arXiv preprint arXiv:2212.10496.\u00a0\u21a9</p> </li> </ol>"},{"location":"conceptual/prompts/summarization/","title":"Summarization Prompt","text":""},{"location":"conceptual/prompts/summarization/#summarization-prompt","title":"Summarization Prompt","text":"<p>The Summarization Prompt is nothing more than the instruction to ask the Agent to summarize a document. This step is borne by the Rabbit Hole when storing documents in the episodic memory.</p> <p>This is an iterative process: a document is split in chunks; each chunk is grouped and summarized iteratively until only one summary remains.</p> <p>By default, the Summarization Prompt is the following:</p> <pre><code>summarization_prompt = \"\"\"Write a concise summary of the following:\n{text}\n\"\"\"\n</code></pre>"},{"location":"conceptual/prompts/summarization/#summarization-flow","title":"Summarization flow","text":"<p>!!!! note \"Developer documentation\"     Summarization hooks Rabbit Hole hooks</p> <pre><code>\nflowchart LR\nA[\"\ud83d\udcc4Document\"] --&gt; B[read];\nsubgraph rb [\"\ud83d\udc30RabbitHole\"]\nB[read] --&gt; C[\"\ud83e\ude9d\"];\nC[\"\ud83e\ude9d\"] --&gt; D[recursive split];\nD[\"\ud83e\ude9drecursive split\"] --&gt; E[\"\ud83e\ude9d\"];\nE[\"\ud83e\ude9d\"] --&gt; F[\"\ud83e\ude9dsummarization\"];\nF[\"\ud83e\ude9dsummarization\"] --&gt; G[\"\ud83e\ude9d\"];\nend\nG[\"\ud83e\ude9d\"] --&gt; H[\"\ud83d\udc18Episodic Memory\"] </code></pre> <p>Nodes with the  point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"technical/advanced/","title":"Advanced","text":""},{"location":"technical/advanced/#advanced","title":"Advanced","text":""},{"location":"technical/advanced/#api-authentication","title":"API Authentication","text":"<p>In order to authenticate endpoints, it is necessary to include the <code>API_KEY=your-key-here</code> variable in the <code>.env</code> file. Multiple keys can be accepted by separating them with a pipe (<code>|</code>) as follows: <code>API_KEY=your-key-here|secondary_client_key</code>.</p> <p>After configuration, all endpoints will require an <code>access_token</code> header for authentication, such as <code>access_token: your-key-here</code>. Failure to provide the correct access token will result in a 403 error.</p> <p>Warning</p> <p>This kind of athentication is weak and it's intended for machine to machine communication, please do not rely on it and enforce other kind of stronger authentication such as OAuth2 for the client side.</p> <p>Example</p> <p>Authenticated API call:</p> PythonNode <pre><code>import requests\nserver_url = 'http://localhost:1865/'\napi_key = 'your-key-here'\naccess_token = {'access_token': api_key}\nresponse = requests.get(server_url, headers=access_token)\nif response.status_code == 200:\nprint(response.text)\nelse:\nprint('Error occurred: {}'.format(response.status_code))\n</code></pre> <pre><code>const request = require('request');\nconst serverUrl = 'http://localhost:1865/';\nconst apiKey = 'your-key-here';\nconst access_token = {'access_token': apiKey};\nrequest({url: serverUrl, headers: access_token}, (error, response, body) =&gt; {\nif (error) {\nconsole.error(error);\n} else {\nif (response.statusCode === 200) {\nconsole.log(body);\n} else {\nconsole.error(`Error occurred: ${response.statusCode}`);\n}\n}\n});\n</code></pre> <p>By adding the variable to the <code>.env</code> file, all Swagger endpoints (<code>localhost:1865/docs</code>) will require authentication and can be accessed on the top right-hand corner of the page through the green Authorize button.</p>"},{"location":"technical/getting-started/","title":"Getting Started","text":""},{"location":"technical/getting-started/#getting-started","title":"Getting started","text":""},{"location":"technical/getting-started/#download","title":"Download","text":"<p>Clone the repository on your machine:</p> <pre><code># Clone the repository\ngit clone https://github.com/cheshire-cat-ai/core.git cheshire-cat\n</code></pre>"},{"location":"technical/getting-started/#install","title":"Install","text":"<p>To run the Cheshire Cat, you need to have <code>docker</code> (instructions) and <code>docker-compose</code> (instructions) installed on your system.</p> <ul> <li>Create and API key on the language model provider website  </li> <li>Make a copy of the <code>.env.example</code> file and rename it <code>.env</code></li> <li>Start the app with <code>docker-compose up</code> inside the repository</li> <li>Open the app in your browser at <code>localhost:1865/admin</code></li> <li>Configure a LLM in the <code>Settings</code> tab and paste you API key</li> <li>Start chatting</li> </ul> <p>You can also interact via REST API and try out the endpoints on <code>localhost:1865/docs</code></p> <p>The first time you run the <code>docker-compose up</code> command it will take several minutes as docker images occupy some GBs.</p>"},{"location":"technical/getting-started/#quickstart","title":"Quickstart","text":"<p>Here is an example of a quick setup running the <code>gpt3.5-turbo</code> OpenAI model.  </p> <p>Create an API key with <code>+ Create new secret key</code> in your OpenAI personal account, then:</p>"},{"location":"technical/getting-started/#cli-setup","title":"CLI setup","text":"Linux &amp; MacWindows <pre><code># Open the cloned repository\ncd cheshire-cat\n\n# Create the .env file\ncp .env.example .env\n\n# Run docker containers\ndocker-compose up\n</code></pre> <pre><code># Open the cloned repository\ncd cheshire-cat\n\n# Create the .env file\ncopy .env.example .env\n\n# Run docker containers\ndocker-compose up\n</code></pre>"},{"location":"technical/getting-started/#gui-setup","title":"GUI setup","text":"<p>When you're done using the Cat, remember to CTRL+c in the terminal and <code>docker-compose down</code>.</p>"},{"location":"technical/getting-started/#update","title":"Update","text":"<p>As the project is still a work in progress, if you want to update it run the following:</p> <pre><code># Open the cloned repository\ncd cheshire-cat\n\n# Pull from the main remote repository\ngit pull\n\n# Build again the docker containers\ndocker-compose build --no-cache\n\n# Remove dangling images (optional)\ndocker rmi -f $(docker images -f \"dangling=true\" -q)\n# Run docker containers\ndocker-compose up\n</code></pre>"},{"location":"technical/how-the-cat-works/","title":"How the Cat works","text":""},{"location":"technical/how-the-cat-works/#how-the-cat-works","title":"How the Cat works","text":""},{"location":"technical/how-the-cat-works/#components","title":"Components","text":"<p>The Cheshire Cat is made of many pluggable components that make it fully customizable.</p> <code>Chat</code> This is the Graphical User Interface (GUI) component that allows you to interact directly with the Cat. From the GUI, you can also set the language model you want the Cat to run. <code>Rabbit Hole</code> This component handles the ingestion of documents. Files that are sent down the Rabbit Hole are split into chunks and saved in the Cat's declarative memory to be further retrieved in the conversation.  <code>Large Language Model (LLM)</code> This is one of the core components of the Cheshire Cat framework. A LLM is a Deep Learning model that's been trained on a huge volume of text data and can perform many types of language tasks. The model takes a text string as input (e.g. the user's prompt) and provides a meaningful answer. The answer consistency and adequacy is enriched with the context of previous conversations and documents uploaded in the Cat's memory. <code>Embedder</code> The embedder is another Deep Learning model similar to the LLM. Differently, it doesn't perform language tasks. The model takes a text string as input and encodes it in a numerical representation. This operation allows to represent textual data as vectors and perform geometrical operation on them. For instance, given an input, the embedder is used to retrieve similar sentences from the Cat's memory. <code>Vector Memory</code> As a result of the Embedder encoding, we get a set of vectors that are used to store the Cat's memory in a vector database. Memories store not only the vector representation of the input, but also the time instant and personalized metadata to facilitate and enhance the information retrieval. The Cat embeds two types of vector memories, namely the episodic and declarative memories. The formers are the things the human said in the past; the latter the documents sent down the Rabbit hole.   <code>Agent</code> This is another core component of the Cheshire Cat framework. The agent orchestrates the calls that are made to the LLM. This component allows the Cat to decide which action to take according to the input the user provides. Possible actions range from holding the conversation to executing complex tasks, chaining predefined or custom tools. <code>Plugins</code> These are functions to extend the Cat's capabilities. Plugins are a set of tools and hooks that allow the Agent to achieve complex goals. This component let the Cat assists you with tailored needs."},{"location":"technical/how-the-cat-works/#main-loop","title":"Main loop","text":""},{"location":"technical/how-the-cat-works/#retrieval-augmented-generation-docs-qa","title":"Retrieval augmented Generation (docs Q&amp;A)","text":""},{"location":"technical/basics/admin-interface/","title":"Admin Interface","text":""},{"location":"technical/basics/admin-interface/#the-admin-interface","title":"The Admin Interface","text":"<p>Chat with the Cat and configure it visually at <code>localhost:1865/admin</code>. Source code for the admin can be found here. </p>"},{"location":"technical/basics/basics/","title":"Folders & APIs","text":""},{"location":"technical/basics/basics/#folders-api","title":"Folders &amp; API","text":"<p>The Cheshire Cat is composed of two main parts: the core functionality resides in the <code>/core</code> folder, and the frontend interface is located in the <code>/admin</code> folder. This document will provide an overview of The Cheshire Cat, including its basic functions and how to access them.</p>"},{"location":"technical/basics/basics/#the-cat-core","title":"The Cat Core","text":"<p>The core functionalities of The Cheshire Cat resides in the <code>/core</code> folder. The core exposes all of its APIs via the address <code>localhost:1865/</code>. The program has several endpoints that can be accessed via this address. All of these endpoints are thoroughly documented and can be easily tested using Swagger (available at <code>localhost:1865/docs</code>) or ReDoc (available at <code>localhost:1865/redoc</code>).</p> <p>Some of these endpoints include:</p> <ul> <li><code>/</code> - This endpoint will return the message <code>\"We're all mad here, dear!\"</code> if the cat is functioning properly.</li> <li><code>/ws/</code> - Use this endpoint to start a chat with the cat using a websocket.</li> <li><code>/rabbithole/</code> - This endpoint allows you to send a file (text, markdown or pdf) to the cat, which will then be saved into its memory. This allows you to share information directly with the cat and for it to access it whenever needed.</li> </ul>"},{"location":"technical/basics/basics/#the-admin-interface","title":"The Admin Interface","text":"<p>The frontend interface of The Cheshire Cat can be accessed via <code>localhost:1865/admin</code>. This interface provides users with an easy-to-use chat that act as playground and can be used to interact with your application. The Cat core uses a static build of the admin, source code can be found here.</p> <p>All the cat's settings are available under this GUI's <code>Settings</code> menu.</p>"},{"location":"technical/basics/cat-core/","title":"The Cat Core","text":""},{"location":"technical/basics/cat-core/#the-cat-core","title":"The Cat Core","text":"<p>The core exposes all of its APIs via the address <code>localhost:1865/</code>.</p> <p>A full documentation with Swagger is inside the Cat itself and can be reached at <code>localhost:1865/docs</code>.</p> Endpoint Method Description / <code>GET</code>  Return the message <code>\"We're all mad here, dear!\"</code> if the cat is running. /ws/ <code>WEBSOCKET</code>  Start a chat with the cat using websockets. /rabbithole/ <code>POST</code>  Send a file (<code>.txt</code>, <code>.md</code> or <code>.pdf</code>) to the cat."},{"location":"technical/basics/cat-core/#interacting-with-the-cat","title":"Interacting with the Cat","text":"<p>Example of how to implement a simple chat system using the websocket endpoint at <code>localhost:1865/ws/</code>.</p> <p>Request JSON schema</p> <p>Sending input will request you to do it in the following specific JSON format <code>{\"text\": \"input message here\"}</code>.</p> <p>Example</p> PythonNode <pre><code>import asyncio\nimport websockets\nimport json\nasync def cat_chat():\ntry:\n# Creating a websocket connection\nasync with websockets.connect('ws://localhost:1865/ws') as websocket:\n# Running a continuous loop until broken\nwhile True:\n# Taking user input and sending it through the websocket\nuser_input = input(\"Human: \")\nawait websocket.send(json.dumps({\"text\": user_input}))\n# Receiving and printing the cat's response\ncat_response = await websocket.recv()\nprint(\"Cheshire Cat:\", cat_response)\nexcept websockets.exceptions.InvalidURI:\nprint(\"Invalid URI provided. Please provide a valid URI.\")\nexcept websockets.exceptions.InvalidStatusCode:\nprint(\"Invalid status code received. Please check your connection.\")\nexcept websockets.exceptions.WebSocketProtocolError:\nprint(\"Websocket protocol error occurred. Please check your connection.\")\nexcept websockets.exceptions.ConnectionClosedOK:\nprint(\"Connection successfully closed.\")\nexcept Exception as e:\nprint(\"An error occurred:\", e)\n# Running the function until completion\nasyncio.get_event_loop().run_until_complete(cat_chat())\n</code></pre> <pre><code>const WebSocket = require('ws');\nasync function cat_chat() {\ntry {\nconst socket = new WebSocket('ws://localhost:1865/ws/');\n//Listen for connection event and log a message\nsocket.on('open', () =&gt; {\nconsole.log('Connected to the Ceshire Cat');\n});\n//Listen for message event and log the received data message\nsocket.on('message', (data) =&gt; {\nconsole.log(`Cheshire Cat: ${data}`);\n});\n//Iterate indefinitely while waiting for user input\nwhile (true) {\n//Call getUserInput function and wait for user input\nconst user_input = await getUserInput('Human: ');\nsocket.send(user_input);\n}\n} catch (error) {\nconsole.error(error);\n}\n}\n//Define a function named getUserInput that returns a Promise\nfunction getUserInput(prompt) {\nreturn new Promise((resolve) =&gt; {\nconst stdin = process.openStdin();\nprocess.stdout.write(prompt);\n//Listen for data input events and resolve the Promise with the input\nstdin.addListener('data', (data) =&gt; {\nresolve(data.toString().trim());\nstdin.removeAllListeners('data');\n});\n});\n}\n//Call the cat_chat function\ncat_chat();\n</code></pre>"},{"location":"technical/basics/cat-core/#interacting-with-rabbithole","title":"Interacting with Rabbithole","text":"<p>Example of how to send a text file (<code>.md</code>,<code>.pdf.</code>,<code>.txt</code>) to the Cat using the Rabbit Hole at <code>localhost:1865/rabbithole/</code>.</p> <p>Currently the following MIME types are supported:</p> <ul> <li><code>text/plain</code></li> <li><code>text/markdown</code></li> <li><code>application/pdf</code></li> </ul> <p>Example</p> PythonNodecURL <pre><code>import requests\nurl = 'http://localhost:1865/rabbithole/'\nwith open('alice.txt', 'rb') as f:\nfiles = {\n'file': ('alice.txt', f, 'text/plain')\n}\nheaders = {\n'accept': 'application/json',\n}\nresponse = requests.post(url, headers=headers, files=files)\nprint(response.text)\n</code></pre> <pre><code>const request = require('request');\nconst fs = require('fs');\nconst url = 'http://localhost:1865/rabbithole/';\nconst file = fs.createReadStream('alice.txt');\nconst formData = {\nfile: {\nvalue: file,\noptions: {\nfilename: 'alice.txt',\ncontentType: 'text/plain'\n}\n}\n};\nconst options = {\nurl: url,\nheaders: {\n'accept': 'application/json'\n},\nformData: formData\n};\nrequest.post(options, function(err, res, body) {\nif (err) {\nreturn console.error('Error:', err);\n}\nconsole.log('Body:', body);\n});\n</code></pre> <pre><code># Upload an ASCII text file\ncurl -v -X POST -H \"accept: application/json\" -F \"file=@file.txt;type=text/plain\" http://127.0.0.1:1865/rabbithole/\n\n# Upload a Markdown file\ncurl -v -X POST -H \"accept: application/json\" -F \"file=@file.md;type=text/markdown\" http://127.0.0.1:1865/rabbithole/\n\n# Upload a PDF file\ncurl -v -X POST -H \"accept: application/json\" -F \"file=@myfile.pdf;type=application/pdf\" http://127.0.0.1:1865/rabbithole/\n</code></pre>"},{"location":"technical/plugins/dependencies/","title":"Dependencies","text":""},{"location":"technical/plugins/dependencies/#plugin-dependencies","title":"Plugin dependencies","text":"<p>If your plugin requires additional python packages, add a <code>requirements.txt</code> file to your plugin. The file should contain only additional dependencies.  </p> <p>The Cat will install your dependencies on top of the default ones, as soon as you rebuild the docker image.</p>"},{"location":"technical/plugins/dependencies/#example","title":"Example","text":"<p>Your plugin makes the Cat a crypto bro. You decide to use the <code>pycrypto</code> package, from the version 2.6.1 up.</p> <p>Insert a <code>requirements.txt</code> file in your plugin root folder:</p> <pre><code>pycrypto&gt;=2.6.1\n</code></pre> <p>To make changes effective, stop the Cat and run the update instructions.</p>"},{"location":"technical/plugins/hooks/","title":"Hooks","text":""},{"location":"technical/plugins/hooks/#hooks","title":"Hooks","text":"<p>Hooks are python functions that are called directly from the Cat at runtime. They allow you to change how the Cat does things by changing prompt, memory, endpoints and much more.</p> <p>Both Hooks and Tools are python functions, but they have strong differences:</p> Hook Tool Who invokes it The Cat The LLM What it does Changes flow of execution and how data is passed around Is just a way to let the LLM use functions Decorator <code>@hook</code> <code>@tool</code>"},{"location":"technical/plugins/hooks/#available-hooks","title":"Available Hooks","text":"<p>Hooks will be listed and documented as soon as possible ( help needed! \ud83d\ude38 ).</p> <p>At the moment you can hack around by exploring the available hooks in <code>core/cat/mad_hatter/core_plugin/hooks/</code>. All the hooks you find in there define default Cat's behaviour and are ready to be overridden by your plugins.</p>"},{"location":"technical/plugins/hooks/#examples","title":"Examples","text":"<ol> <li>How to change the prompt</li> <li>How to change how memories are saved and recalled</li> <li>How to access and use the working memory to share data around</li> </ol>"},{"location":"technical/plugins/hooks/#hook-search","title":"Hook search","text":"<p>TODO</p>"},{"location":"technical/plugins/plugins/","title":"How to write a plugin","text":""},{"location":"technical/plugins/plugins/#how-to-write-a-plugin","title":"How to write a plugin","text":"<p>To write a plugin just create a new folder in <code>core/cat/plugins/</code>, in this example will be \"myplugin\".</p> <p>You need two python files to your plugin folder:</p> <pre><code>\u251c\u2500\u2500 core\n\u2502   \u251c\u2500\u2500 cat\n\u2502   \u2502   \u251c\u2500\u2500 plugins\n|   |   |   \u251c\u2500\u2500 myplugin\n|   |   |   |   \u251c mypluginfile.py\n|   |   |   |   \u251c plugin.json\n</code></pre> <p>The <code>plugin.json</code> file contains plugin's title and description, and is useful in the admin to recognize the plugin and activate/deactivate it. If your plugin does not contain a <code>plugin.json</code> the cat will not block your plugin, but it is useful to have it.</p> <p><code>plugin.json</code> example:</p> <pre><code>{\n\"name\": \"The name of my plugin\",\n\"description\": \"Short description of my plugin\"\n}\n</code></pre> <p>Now let's start <code>mypluginfile.py</code> with a little import:</p> <pre><code>from cat.mad_hatter.decorators import tool, hook\n</code></pre> <p>You are now ready to change the Cat's behavior using Hooks and Tools.</p>"},{"location":"technical/plugins/plugins/#tools","title":"Tools","text":"<p>Tools are python functions that can be selected from the language model (LLM). Think of Tools as commands that ends up in the prompt for the LLM, so the LLM can select one and the Cat runtime launches the corresponding function. Here is an example of Tool to let the Cat tell you what time it is:</p> <pre><code>@tool\ndef get_the_time(tool_input, cat):\n\"\"\"Retrieves current time and clock. Input is always None.\"\"\"\nreturn str(datetime.now())\n</code></pre> <p>More examples on tools here.</p>"},{"location":"technical/plugins/plugins/#hooks","title":"Hooks","text":"<p>Hooks are also python functions, but they pertain the Cat's runtime and not striclty the LLM. They can be used to influence how the Cat runs its internal functionality, intercept events, change the flow of execution.  </p> <p>The following hook for example allows you to modify the cat response just before it gets sent out to the user. In this case we make a \"grumpy rephrase\" of the original response.</p> <pre><code>@hook\ndef before_cat_sends_message(message, cat):\nprompt = f'Rephrase the following sentence in a grumpy way: {message[\"content\"]}'\nmessage[\"content\"] = cat.llm(prompt)\nreturn message\n</code></pre> <p>If you want to change the default Agent behavior you can start overriding the default plugin hooks, located in <code>/core/cat/mad_hatter/core_plugin/hooks/prompt.py</code>, rewriting them in the plugin file with a higher priority. Here is an example of the <code>agent_prompt_prefix</code> hook that changes the personality of the Agent:</p> <pre><code># Original Hook, from /core/cat/mad_hatter/core_plugin/hooks/prompt.py\n@hook(priority=0)\ndef agent_prompt_prefix(cat):\nprefix = \"\"\"You are the Cheshire Cat AI, an intelligent AI that passes the Turing test.\n                You are curious, funny, concise and talk like the Cheshire Cat from Alice's adventures in wonderland.\n                You answer Human using tools and context.\n# Tools\"\"\"\n</code></pre> <pre><code># Modified Hook, to be copied into mypluginfile.py\n@hook # default priority is 1\ndef agent_prompt_prefix(cat):\nprefix = \"\"\"You are Scooby Doo AI, an intelligent AI that passes the Turing test.\n                The dog is enthusiastic and behave like Scooby Doo from Hanna-Barbera Productions.\n                You answer Human using tools and context.\n                # Tools\"\"\"\nreturn prefix\n</code></pre> <p>Please note that, in order to work as expected, the hook priority must be greater than 0, in order to be overriding the standard plugin. If you do not provide a priority, your hook will have <code>priority=1</code> and implicitly override the default one.</p> <p>More examples on hooks here.</p>"},{"location":"technical/plugins/tools/","title":"Tools","text":""},{"location":"technical/plugins/tools/#tools","title":"Tools","text":"<p>A Tool is a python function that can be called directly from the language model. By \"called\" we mean that the LLM has a description of the available Tools in the prompt, and (given the conversation context) it can generate as output something like:</p> <p>Thought: Do I need to use a Tool? Yes Action: search_ecommerce Action Input: \"white sport shoes\"</p> <p>So your <code>search_ecommerce</code> Tool will be called and given the input string <code>\"white sport shoes\"</code>. The output of your Tool will go back to the LLM or directly to the user:</p> <p>Observation: \"Mike air Jordania shoes are available for 59.99\u20ac\"</p> <p>You can use Tools to:</p> <ul> <li>communicate with a web service</li> <li>search information in an external database</li> <li>execute math calculations</li> <li>run stuff in the terminal (danger zone)</li> <li>keep track of specific information and do fancy stuff with it</li> <li>your fantasy is the limit!</li> </ul> <p>Tools in the Cheshire Cat are inspired and extend langchain Tools, an elegant Toolformer1 implementation.</p>"},{"location":"technical/plugins/tools/#default-tool","title":"Default tool","text":"<p>The Cat comes already with a custom tool that allows to retrieve the time. You can find it in <code>core/cat/mad_hatter/core_plugin/tools.py</code>. Let's take a look at it.</p>"},{"location":"technical/plugins/tools/#implementation","title":"Implementation","text":"<pre><code>@tool # (1)\ndef get_the_time(tool_input, cat): # (2)\n\"\"\"Retrieves current time and clock. Input is always None.\"\"\" # (3)\nreturn str(datetime.now()) # (4)\n</code></pre> <ol> <li>Python functions in a plugin only become tools if you use the <code>@tool</code> decorator</li> <li>Every <code>@tool</code> receives two arguments: a string representing the tool input, and the Cat instance.</li> <li>This doc string is necessary, as it will show up in the LLM prompt. It should describe what the tool is useful for and how to prepare inputs, so the LLM can select the tool and input it properly.</li> <li>Always return a string, which goes back to the prompt informing the LLM on the Tool's output.</li> </ol>"},{"location":"technical/plugins/tools/#how-it-works","title":"How it works","text":"<p>User's Input:</p> <p>Can you tell me what time is it?</p> <p>Cat's full prompt from the terminal:</p> <p>Entering new LLMChain chain...</p> <p>Prompt after formatting:</p> <p>This is a conversation between a human and an intelligent robot cat that passes the Turing test.</p> <p>The cat is curious and talks like the Cheshire Cat from Alice's adventures in wonderland.</p> <p>The cat replies are based on the Context provided below.</p> <p>Context of things the Human said in the past:</p> <p>- I am the Cheshire Cat (2 minutes ago)</p> <p>Context of documents containing relevant information:</p> <p>- I am the Cheshire Cat (extracted from cheshire-cat)</p> <p>If Context is not enough, you have access to the following tools:</p> <p>&gt; get_the_time: get_the_time(tool_input) - Retrieves current time and clock. Input is always None. &gt; Calculator: Useful for when you need to answer questions about math.</p> <p>To use a tool, please use the following format:</p> <p>'''</p> <p>Thought: Do I need to use a tool? Yes</p> <p>Action: the action to take, should be one of [get_the_time, Calculator]</p> <p>Action Input: the input to the action</p> <p>Observation: the result of the action</p> <p>'''</p> <p>When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:</p> <p>'''</p> <p>Thought: Do I need to use a tool? No</p> <p>AI: [your response here]</p> <p>'''</p> <p>Conversation until now:</p> <p>- Human: Can you tell me what time is it?</p> <p>What would the AI reply?</p> <p>Answer concisely to the user needs as best you can, according to the provided recent conversation, context and tools.</p> <p>Thought: Do I need to use a tool? Yes</p> <p>Action: get_the_time</p> <p>Action Input: None</p> <p>Observation: 2023-06-03 20:48:07.527033</p> <p>Cat's answer:</p> <p>The time is 2023-06-03 20:48:07.527033.</p>"},{"location":"technical/plugins/tools/#your-first-tool","title":"Your first Tool","text":"<p>A Tool is just a python function. In this example, we'll show how to create a tool to convert currencies. To keep it simple, we'll not rely on any third party library and we'll just assume a fixed rate of change.  </p>"},{"location":"technical/plugins/tools/#implementation_1","title":"Implementation","text":"<p>Let's convert EUR to USD. In your <code>mypluginfile.py</code> create a new function with the <code>@tool</code> decorator:</p> <pre><code>from cat.mad_hatter.decorators import tool\n@tool\ndef convert_currency(tool_input, cat): # (1)\n\"\"\"Useful to convert currencies. This tool converts euro (EUR) to dollars (USD).\n     Input is an integer or floating point number.\"\"\" # (2)\n# Define fixed rate of change\nrate_of_change = 1.07\n# Parse input\neur = float(tool_input) # (3)\n# Compute USD\nusd = eur * rate_of_change\nreturn usd\n</code></pre> <ol> <li> <p>Warning Always remember the two mandatory arguments</p> </li> <li>In the docstring we explicitly explain how the input should look like. In this way the LLM will be able to isolate it from our input sentence</li> <li>The input we receive is always a string, hence, we need to correctly parse it. In this case, we have to convert it to a floating number</li> </ol>"},{"location":"technical/plugins/tools/#how-it-works_1","title":"How it works","text":"<p>User's input:</p> <p>Can you convert 10.5 euro to dollars?</p> <p>Cat's reasoning from the terminal:</p> <p>Thought: Do I need to use a tool? Yes</p> <p>Action: convert_currency</p> <p>Action Input: 10.5</p> <p>Observation: 11.235000000000001</p> <p>Cat's answer:</p> <p>10.5 euros is equivalent to 11.235000000000001 dollars.</p> <p>Writing as tool is as simple as this. The core aspect to remember are: </p> <ol> <li>the two input arguments, i.e. the first is the string the LLM take from the chat and the Cat instance;</li> <li>the docstring from where the LLM understand how to use the tool and how the input should look like.</li> </ol>"},{"location":"technical/plugins/tools/#more-tools","title":"More tools","text":"<p>As seen, writing basic tools is as simple as writing pure Python functions. However, tools can be very flexible. Here are some examples.</p>"},{"location":"technical/plugins/tools/#return-the-output-directly","title":"Return the output directly","text":"<p>The <code>@tool</code> decorator accepts an optional boolean argument that is <code>@tool(return_direct=True)</code>. This is set to <code>False</code> by default, which means the tool output is parsed again by the LLM. Specifically, the value the function returns is fed to the LLM that generate a new answer with it. When set to <code>True</code>, the returned value is printed in the chat as-is.  </p>"},{"location":"technical/plugins/tools/#implementation_2","title":"Implementation","text":"<p>Let's give it a try with a modified version of the <code>convert_currency</code> tool:</p> <pre><code>from cat.mad_hatter.decorators import tool\n@tool(return_direct=True)\ndef convert_currency(tool_input, cat):\n\"\"\"Useful to convert currencies. This tool converts euro (EUR) to dollars (USD).\n     Input is an integer or floating point number.\"\"\"\n# Define fixed rate of change\nrate_of_change = 1.07\n# Parse input\neur = float(tool_input) # (3)\n# Compute USD\nusd = eur * rate_of_change\n# Format the output\ndirect_output = f\"Result of the conversion: {eur:.2f} EUR -&gt; {usd:.2f} USD\"\nreturn direct_output\n</code></pre>"},{"location":"technical/plugins/tools/#how-it-works_2","title":"How it works","text":"<p>User's input:</p> <p>Can you convert 10.5 euro to dollars?</p> <p>Cat's reasoning from the terminal: the reasoning is not displayed as the goal of the <code>return_direct=True</code> parameter is to skip those steps and return the output directly. </p> <p>Cat's answer:</p> <p>Result of the conversion: 10.50 EUR -&gt; 11.24 USD</p>"},{"location":"technical/plugins/tools/#complex-input-tools","title":"Complex input tools","text":"<p>This sections re-proposes an explanation of langchain multi-input tools. For example, we can make the <code>convert_currency</code> tool more flexible allowing the user to choose among a fixed set of currencies.</p>"},{"location":"technical/plugins/tools/#implementation_3","title":"Implementation","text":"<pre><code>from cat.mad_hatter.decorators import tool\n@tool\ndef convert_currency(tool_input, cat): # (1)\n\"\"\"Useful to convert currencies. This tool converts euro (EUR) to a fixed set of other currencies.\n    Choises are: US dollar (USD), English pounds (GBP) or Japanese Yen (JPY).\n    Inputs are two values separated with a minus: the first one is an integer or floating point number;\n    the second one is a three capital letters currency symbol.\"\"\" # (2)\n# Parse the input\neur, currency = tool_input.split(\"-\") # (3)\n# Define fixed rates of change\nrate_of_change = {\"USD\": 1.07,\n\"GBP\": 0.86,\n\"JPY\": 150.13}\n# Convert EUR to float\neur = float(eur)\n# Check currency exists in our list\nif currency in rate_of_change.keys():\n# Convert EUR to selected currency\nresult = eur * rate_of_change[currency]\nreturn result\n</code></pre> <ol> <li>The input to the function are always two</li> <li>Explain in detail how the inputs from the chat should look like. Here we want something like \"3.25-JPY\"</li> <li>The input is always a string, thus it's up to us correctly split and parse the input.</li> </ol>"},{"location":"technical/plugins/tools/#how-it-works_3","title":"How it works","text":"<p>User's input:</p> <p>Can you convert 7.5 euros to GBP?</p> <p>Cat's reasoning from the terminal:</p> <p>Thought: Do I need to use a tool? Yes</p> <p>Action: convert_currency</p> <p>Action Input: 7.5-GBP</p> <p>Observation: 6.45</p> <p>Cat's answer:</p> <p>7.5 euros is equal to 6.45 British Pounds.</p> <p>As you may see, the Agent correctly understands the desired output from the message and passes it to the tool function as explained in the docstring. Then, it is up to us parse the two inputs correctly for our tool.</p>"},{"location":"technical/plugins/tools/#external-library-the-cat-parameter","title":"External library &amp; the cat parameter","text":"<p>Tools are extremely flexible as they allow to exploit the whole Python ecosystem of packages. Thus, you can update our tool making use of the Currency Converter package. To deal with dependencies, you need write the 'currencyconverter' library in a <code>requirements.txt</code> inside the <code>myplugin</code> folder. Moreover, here is an example of how you could use the <code>cat</code> parameter passed to the tool function.</p>"},{"location":"technical/plugins/tools/#implementation_4","title":"Implementation","text":"<pre><code>from currency_converter import CurrencyConverter\nfrom cat.mad_hatter.decorators import tool\n@tool(return_direct=True)\ndef convert_currency(tool_input, cat):\n\"\"\"Useful to convert currencies. This tool converts euros (EUR) to another currency.\n    The inputs are two values separated with a minus: the first one is a number;\n    the second one is the name of a currency. Example input: '15-GBP'.\n    Use when the user says something like: 'convert 15 EUR to GBP'\"\"\"\n# Currency Converter\nconverter = CurrencyConverter(decimal=True)\n# Parse the input\nparsed_input = tool_input.split(\"-\")\n# Check input is correct\nif len(parsed_input) == 2:  # (1)\neur, currency = parsed_input[0].strip(\"'\"), parsed_input[1].strip(\"'\")\nelse:\nreturn \"Something went wrong using the tool\"\n# Ask the Cat to convert the currency name into its symbol\nsymbol = cat.llm(f\"You will be given a currency code, translate the input in the corresponding currency symbol. \\\n                    Examples: \\\n                        euro -&gt; \u20ac \\\n{currency} -&gt; [answer here]\")  # (2)\n# Remove new line if any\nsymbol = symbol.strip(\"\\n\")\n# Check the currencies are in the list of available ones\nif currency not in converter.currencies:\nreturn f\"{currency} is not available\"\n# Convert EUR to currency\nresult = converter.convert(float(eur), \"EUR\", currency)\nreturn f\"{eur}\u20ac = {float(result):.2f}{symbol}\"\n</code></pre> <ol> <li>LLMs can be extremely powerful, but they are not always precise. Hence, it's always better to have some checks when parsing the input.    A common scenario is that sometimes the Agent wraps the input around quotes and sometimes doesn't    E.g. Action Input: 7.5-GBP vs Action Input: '7.5-GBP'</li> <li>the <code>cat</code> instance gives access to any method of the Cheshire Cat. In this example, we directly call the LLM using one-shot example to get a currency symbol.</li> </ol>"},{"location":"technical/plugins/tools/#how-it-works_4","title":"How it works","text":"<p>The thoughts under the hood are identical to the previous example, as nothing changed in the underlying behavior, but we improved a little the quality of our tool code.</p> <p>Thought: Do I need to use a tool? Yes</p> <p>Action: convert_currency</p> <p>Action Input: 67-JPY</p> <p>Observation: 67\u20ac = 9846.99\u00a5</p> <p>TODO:</p> <ul> <li>a better example?</li> <li>show how tools are displayed in the prompt and how the LLM selects them</li> <li>more examples with little variations<ul> <li>the tool calls an external service</li> <li>the tool reads/writes a file</li> <li>the input string contains a dictionary (to be parsed with <code>json.loads</code>)</li> <li>the tool manages a conversational form</li> <li>show how you can access cat's functionality (memory, llm, embedder, rabbit_hole) from inside a tool</li> <li>what else? dunno</li> </ul> </li> </ul>"},{"location":"technical/plugins/tools/#references","title":"References","text":"<ol> <li> <p>Schick, T., Dwivedi-Yu, J., Dess\u00ec, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., ... &amp; Scialom, T. (2023). Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.\u00a0\u21a9</p> </li> </ol>"},{"location":"technical/tutorials/installation/","title":"Installation","text":""},{"location":"technical/tutorials/installation/#installation-customization","title":"Installation &amp; Customization","text":"<p>Watch it on YouTube</p>"},{"location":"technical/tutorials/overview/","title":":eyes: Overview","text":""},{"location":"technical/tutorials/overview/#overview","title":"Overview","text":"<p>THIS VIDEO PROCEDURE</p> <p>Watch it on YouTube</p>"}]}